---
title: "Data Modeling"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

```{r}
# Load libraries
library(tidyverse)
library(pROC)
library(glmnet)
library(randomForest)
library(gt)
library(doParallel)
library(caret)

options(na.action = na.pass)

data_dir <- "data-files"

# Read datasets
train <- read_csv(file.path(data_dir, "train_final.csv")) |>
  mutate(across(where(is.character), as.factor))

test <- read_csv(file.path(data_dir, "test_final.csv")) |>
  mutate(across(where(is.character), as.factor))
```

# Majority Class Model

```{r}
TARGET_dist <- train |> 
  count(TARGET) |>
  mutate(proportion = n / sum(n))

gt(TARGET_dist)
```

```{r}
# Baseline predictions (majority class)
baseline_predictions <- rep.int(0, nrow(train))
baseline_accuracy <- mean(baseline_predictions == train$TARGET)

# Baseline probabilities (probability of TARGET = 1)
majority_class_prob <- TARGET_dist$proportion[TARGET_dist$TARGET == 0]
baseline_probs <- rep(majority_class_prob, nrow(train))

baseline_auc <- roc(train$TARGET, baseline_probs)$auc
baseline_results <- tibble(
  Metric = c("Baseline Accuracy", "Baseline AUC"),
  Value = c(baseline_accuracy, as.numeric(baseline_auc))
)

gt(baseline_results)
```

The majority class (TARGET = 0) represents approximately 91.9% of the training data, indicating a significant class imbalance. A baseline model that always predicts the majority class would achieve an accuracy of around 91.9%. However, this model would have an AUC of 0.5, as it does not differentiate between the classes.

# Compare Candidate Models

## Data Preparation
```{r}
raw_app_train <- read_csv(file.path(data_dir, "application_train.csv")) |>
  mutate(across(where(is.character), as.factor))

# Keep only application-based predictors
application_cols <- colnames(raw_app_train)
application_cols_in_train <- intersect(application_cols, colnames(train))

train_app <- train |>
  select(all_of(application_cols_in_train), TARGET, -SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

The model comparison will focus on the original application data. Supplementary data sources (bureau, previous applications, etc.) will be explored in future iterations. The LASSO model will be trained using only numeric predictors.

## Cross-Validation Control

```{r}
set.seed(42)
n_folds <- 5
fold_ids <- sample(rep(1:n_folds, length.out = nrow(train_app)))

# Storage for fold predictions
preds_lr <- numeric(nrow(train_app))
preds_lasso <- numeric(nrow(train_app))
preds_rf <- numeric(nrow(train_app))

# Storage for number of predictors
n_pred_lr <- n_pred_lasso <- n_pred_rf <- NA
```

## Enable Parallel Processing

```{r}
n_cores <- 2
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl)
```

## Modeling


```{r}
for (k in 1:n_folds) {
  cat("Processing fold", k, "\n")
  
  train_fold <- train_app[fold_ids != k, ]
  test_fold  <- train_app[fold_ids == k, ]
  
  # Logistic Regression
  fit_lr <- glm(TARGET ~ ., data = train_fold, family = binomial)
  preds_lr[fold_ids == k] <- predict(fit_lr, test_fold, type = "response")
  if (is.na(n_pred_lr)) n_pred_lr <- length(coef(fit_lr)) - 1
  
  # LASSO Logistic Regression
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, parallel = TRUE)
  pred_lasso_fold <- predict(lasso, x_test, s = "lambda.min", type = "response")
  preds_lasso[fold_ids == k] <- pred_lasso_fold
  if (is.na(n_pred_lasso)) {
    coefs <- coef(lasso, s = "lambda.min")
    n_pred_lasso <- sum(coefs[-1, ] != 0)
  }
  
  # Random Forest
  set.seed(42 + k)
  rf <- randomForest(TARGET ~ ., data = train_fold, ntree = 200)
  pred_rf_fold <- predict(rf, test_fold, type = "prob")[, "1"]
  preds_rf[fold_ids == k] <- pred_rf_fold
  if (is.na(n_pred_rf)) n_pred_rf <- ncol(train_fold) - 1
}
```

## Stop Parallel Cluster

 ```{r}
stopCluster(cl)
registerDoSEQ()
```

## Model Comparison

### Compute Cross-Validated AUC

```{r}
auc_lr <- auc(roc(train_app$TARGET, preds_lr))
auc_lasso <- auc(roc(train_app$TARGET, preds_lasso))
auc_rf <- auc(roc(train_app$TARGET, preds_rf))
```

### Candidate Model Summary Table

```{r}
comparison_table <- tibble(
  Model = c("Logistic", "LASSO", "Random Forest"),
  CV_AUC = c(auc_lr, auc_lasso, auc_rf),
  Num_Predictors = c(n_pred_lr, n_pred_lasso, n_pred_rf)
)

gt(comparison_table)
```

We will use the LASSO model, because it achieved the highest cross-validated AUC of the three models, followed closely by the Logistic Regression, then the Random Forest. The LASSO model also used significantly fewer predictors than the full Logistic Regression model.

# Model Optimization

## Add Supplementary Features to LASSO Model

```{r}
# full training data (application + supplementary features)
train_model <- train |> select(-SK_ID_CURR)

# recode TARGET levels
train_model$TARGET <- factor(train_model$TARGET, levels = c(0,1), labels = c("No","Yes"))
```

## Fit LASSO Model with SMOTE

```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  sampling = "smote",            # address class imbalance
  classProbs = TRUE,         
  summaryFunction = twoClassSummary
)

set.seed(42)
lasso_caret <- train(
  TARGET ~ .,
  data = train_model,
  method = "glmnet",
  family = "binomial",
  metric = "ROC",
  trControl = ctrl
)
```

## Predict on Test Set 
```{r}
## Predict on Test Set
test_model <- test |> select(-SK_ID_CURR)
pred_test_full <- predict(lasso_caret, newdata = test_model, type = "prob")[, "Yes"]
```

# Create Submission File
```{r}
submission_full <- tibble(
  SK_ID_CURR = test$SK_ID_CURR,
  TARGET = pred_test_full
)

write_csv(submission_full, "submission_lasso_full.csv")
```