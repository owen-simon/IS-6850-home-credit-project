---
title: "Data Modeling"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

```{r}
# Load libraries
library(tidyverse)
library(pROC)
library(glmnet)
library(randomForest)
library(gt)
library(doParallel)

options(na.action = na.pass)

data_dir <- "data-files"

# Read datasets
train <- read_csv(file.path(data_dir, "train_final.csv")) |>
  mutate(across(where(is.character), as.factor))

test <- read_csv(file.path(data_dir, "test_final.csv")) |>
  mutate(across(where(is.character), as.factor))
```

# Majority Class Model

```{r}
TARGET_dist <- train |> 
  count(TARGET) |>
  mutate(proportion = n / sum(n))

gt(TARGET_dist)
```

```{r}
# Baseline predictions (majority class)
baseline_predictions <- rep.int(0, nrow(train))
baseline_accuracy <- mean(baseline_predictions == train$TARGET)

# Baseline probabilities (probability of TARGET = 1)
majority_class_prob <- TARGET_dist$proportion[TARGET_dist$TARGET == 0]
baseline_probs <- rep(majority_class_prob, nrow(train))

baseline_auc <- roc(train$TARGET, baseline_probs)$auc
baseline_results <- tibble(
  Metric = c("Baseline Accuracy", "Baseline AUC"),
  Value = c(baseline_accuracy, as.numeric(baseline_auc))
)

gt(baseline_results)
```

The majority class (TARGET = 0) represents approximately 91.9% of the training data, indicating a significant class imbalance. A baseline model that always predicts the majority class would achieve an accuracy of around 91.9%. However, this model would have an AUC of 0.5, as it does not differentiate between the classes.

# Compare Candidate Models

## Data Preparation
```{r}
raw_app_train <- read_csv(file.path(data_dir, "application_train.csv")) |>
  mutate(across(where(is.character), as.factor))

# Keep only application-based predictors
application_cols <- colnames(raw_app_train)
application_cols_in_train <- intersect(application_cols, colnames(train))

train_app <- train |>
  select(all_of(application_cols_in_train), TARGET, -SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

The model comparison will focus on the original application data. Supplementary data sources (bureau, previous applications, etc.) will be explored in future iterations. The LASSO model will be trained using only numeric predictors.

## Cross-Validation Control

```{r}
set.seed(42)
n_folds <- 5
fold_ids <- sample(rep(1:n_folds, length.out = nrow(train_app)))

# Storage for fold predictions
preds_lr <- numeric(nrow(train_app))
preds_lasso <- numeric(nrow(train_app))
preds_rf <- numeric(nrow(train_app))

# Storage for number of predictors
n_pred_lr <- n_pred_lasso <- n_pred_rf <- NA
```

## Enable Parallel Processing

```{r}
n_cores <- 2
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl)
```

## Modeling


```{r}
for (k in 1:n_folds) {
  cat("Processing fold", k, "\n")
  
  train_fold <- train_app[fold_ids != k, ]
  test_fold  <- train_app[fold_ids == k, ]
  
  # Logistic Regression
  fit_lr <- glm(TARGET ~ ., data = train_fold, family = binomial)
  preds_lr[fold_ids == k] <- predict(fit_lr, test_fold, type = "response")
  if (is.na(n_pred_lr)) n_pred_lr <- length(coef(fit_lr)) - 1
  
  # LASSO Logistic Regression
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, parallel = TRUE)
  pred_lasso_fold <- predict(lasso, x_test, s = "lambda.min", type = "response")
  preds_lasso[fold_ids == k] <- pred_lasso_fold
  if (is.na(n_pred_lasso)) {
    coefs <- coef(lasso, s = "lambda.min")
    n_pred_lasso <- sum(coefs[-1, ] != 0)
  }
  
  # Random Forest
  set.seed(42 + k)
  rf <- randomForest(TARGET ~ ., data = train_fold, ntree = 200)
  pred_rf_fold <- predict(rf, test_fold, type = "prob")[, "1"]
  preds_rf[fold_ids == k] <- pred_rf_fold
  if (is.na(n_pred_rf)) n_pred_rf <- ncol(train_fold) - 1
}
```

## Stop Parallel Cluster

 ```{r}
stopCluster(cl)
registerDoSEQ()
```

## Model Comparison

### Compute Cross-Validated AUC

```{r}
auc_lr <- auc(roc(train_app$TARGET, preds_lr))
auc_lasso <- auc(roc(train_app$TARGET, preds_lasso))
auc_rf <- auc(roc(train_app$TARGET, preds_rf))
```

### Candidate Model Summary Table

```{r}
comparison_table <- tibble(
  Model = c("Logistic", "LASSO", "Random Forest"),
  CV_AUC = c(auc_lr, auc_lasso, auc_rf),
  Num_Predictors = c(n_pred_lr, n_pred_lasso, n_pred_rf)
)

gt(comparison_table)
```

The LASSO model achieves the highest cross-validated AUC of the three models, followed closely by the Logistic Regression, then the Random Forest. The LASSO model also uses significantly fewer predictors than the standard Logistic Regression.

# Model Optimization

## Add Supplementary Data to LASSO Model

```{r}
# Ensure TARGET is properly formatted
train$TARGET <- factor(train$TARGET, levels = c(0,1))

# Remove ID column only
train_full <- train |>
  select(-SK_ID_CURR)

# Create model matrix (dummy encode everything automatically)
x_train <- model.matrix(TARGET ~ . - 1, data = train_full)
y_train <- as.numeric(train_full$TARGET) - 1
```

## Fit LASSO Model with Full Data

```{r}
set.seed(42)
final_lasso_full <- cv.glmnet(
  x_train,
  y_train,
  family = "binomial",
  alpha = 1,
  parallel = TRUE
)
```

## Prepare Full Test Set for Prediction

```{r}
test_full <- test_final |>
  select(-SK_ID_CURR)

x_test <- model.matrix(~ . - 1, data = test_full)
```

## Align Columns

```{r}
# Add missing columns in test
missing_cols <- setdiff(colnames(x_train), colnames(x_test))

if(length(missing_cols) > 0){
  x_test <- cbind(
    x_test,
    matrix(0, nrow(x_test), length(missing_cols),
           dimnames = list(NULL, missing_cols))
  )
}

# Reorder to match training matrix
x_test <- x_test[, colnames(x_train)]
```

## Generate Predictions on Test Set

```{r}
pred_test_full <- predict(
  final_lasso_full,
  x_test,
  s = "lambda.min",
  type = "response"
)
```

## Create Submission File

```{r}
submission_full <- tibble(
  SK_ID_CURR = test$SK_ID_CURR,
  TARGET = as.numeric(pred_test_full)
)

write_csv(submission_full, "submission_lasso_full.csv")
```