---
title: "Data Modeling"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

# 1. Setup

```{r}
# Load libraries
library(tidyverse)
library(pROC)
library(glmnet)
library(randomForest)
library(gt)
library(doParallel)
library(caret)
library(tibble)

options(na.action = na.pass)

data_dir <- "data-files"

# Read datasets
train <- read_csv(file.path(data_dir, "train_final.csv")) |>
  mutate(across(where(is.character), as.factor))

test <- read_csv(file.path(data_dir, "test_final.csv")) |>
  mutate(across(where(is.character), as.factor))
```

# 2. Majority Class Model

## 2.1 Overview

```{r}
TARGET_dist <- train |> 
  count(TARGET) |>
  mutate(proportion = n / sum(n))

gt(TARGET_dist)
```

The target variable is highly imbalanced, with the majority class (TARGET = 0) representing approximately 91.9% of the training data. This imbalance will be an important consideration when evaluating model performance.

## 2.2 Baseline Model Performance

```{r}
# Baseline predictions (majority class)
baseline_predictions <- rep.int(0, nrow(train))
baseline_accuracy <- mean(baseline_predictions == train$TARGET)

# Baseline probabilities (probability of TARGET = 1)
positive_class_prob <- TARGET_dist$proportion[TARGET_dist$TARGET == 1]
baseline_probs <- rep(positive_class_prob, nrow(train))

baseline_auc <- roc(train$TARGET, baseline_probs, levels = c(0,1))$auc
baseline_results <- tibble(
  Metric = c("Baseline Accuracy", "Baseline AUC"),
  Value = c(baseline_accuracy, as.numeric(baseline_auc))
)

gt(baseline_results)
```

A baseline model that always predicts the majority class achieves an accuracy of approximately 91.9% but an AUC of 0.5, indicating it cannot distinguish between the classes.

This baseline provides a reference point for evaluating predictive models, which should outperform the simple majority-class classifier to demonstrate meaningful predictive power.

# 3. Compare Candidate Models

## 3.1 Data Preparation
```{r}
raw_app_train <- read_csv(file.path(data_dir, "application_train.csv")) |>
  mutate(across(where(is.character), as.factor))

# Keep only application-based predictors
application_cols <- colnames(raw_app_train)
application_cols_in_train <- intersect(application_cols, colnames(train))

train_app <- train |>
  select(all_of(application_cols_in_train), TARGET, -SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

The initial model comparison will focus on the original application data. Supplementary data sources (bureau, previous applications, etc.) will be explored in future iterations.

## 3.2 Cross-Validation Control

```{r}
set.seed(42)
n_folds <- 3
fold_ids <- sample(rep(1:n_folds, length.out = nrow(train_app)))

# Storage for fold predictions
preds_lr <- numeric(nrow(train_app))
preds_lasso <- numeric(nrow(train_app))
preds_rf <- numeric(nrow(train_app))

# Storage for number of predictors
n_pred_lr <- n_pred_lasso <- n_pred_rf <- NA
```

Three fold cross validation is used to estimate out of sample model performance. For each fold, models are trained on two thirds of the training data and evaluated on the remaining one third. This approach provides a reliable estimate of how the models are likely to perform on unseen data while balancing computation time.

## 3.3 Parallel Processing

```{r}
n_cores <- 2
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl)
```

We set up parallel processing to speed up model fitting across folds.

## 3.4 Modeling Folds

```{r}
for (k in 1:n_folds) {
  cat("Processing fold", k, "\n")
  
  train_fold <- train_app[fold_ids != k, ]
  test_fold  <- train_app[fold_ids == k, ]
  
  # Logistic Regression
  fit_lr <- glm(TARGET ~ ., data = train_fold, family = binomial)
  preds_lr[fold_ids == k] <- predict(fit_lr, test_fold, type = "response")
  if (is.na(n_pred_lr)) n_pred_lr <- length(coef(fit_lr)) - 1
  
  # LASSO Logistic Regression
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, parallel = TRUE)
  pred_lasso_fold <- predict(lasso, x_test, s = "lambda.min", type = "response")
  preds_lasso[fold_ids == k] <- pred_lasso_fold
  if (is.na(n_pred_lasso)) {
    coefs <- coef(lasso, s = "lambda.min")
    n_pred_lasso <- sum(coefs[-1, ] != 0)
  }
  
  # Random Forest
  set.seed(42 + k)
  rf <- randomForest(TARGET ~ ., data = train_fold, ntree = 200)
  pred_rf_fold <- predict(rf, test_fold, type = "prob")[, "1"]
  preds_rf[fold_ids == k] <- pred_rf_fold
  if (is.na(n_pred_rf)) n_pred_rf <- ncol(train_fold) - 1
}
```

Each fold trains logistic regression, LASSO, and random forest models, storing out-of-fold predictions for unbiased AUC estimation.

## 3.5 Stop Parallel Cluster

```{r}
stopCluster(cl)
registerDoSEQ()
```

## 3.6 Model Comparison

### 3.6.1 Compute CV-AUC

```{r}
auc_lr <- auc(roc(train_app$TARGET, preds_lr, levels = c(0,1)))
auc_lasso_app <- auc(roc(train_app$TARGET, preds_lasso, levels = c(0,1)))
auc_rf <- auc(roc(train_app$TARGET, preds_rf, levels = c(0,1)))
```

The reported AUC values are cross validated estimates computed from out of fold predictions. Because each observation is evaluated using a model that was not trained on it, these values provide an unbiased estimate of expected performance on unseen data.

### 3.6.2 Candidate Model Summary Table

```{r}
comparison_table <- tibble(
  Model = c("Logistic", "LASSO", "Random Forest"),
  CV_AUC = c(auc_lr, auc_lasso_app, auc_rf),
  Num_Predictors = c(n_pred_lr, n_pred_lasso, n_pred_rf)
)

gt(comparison_table)
```

The LASSO model will be used for further development as it achieves the highest cross-validated AUC among candidate models while retaining fewer predictors than standard Logistic Regression.

# 4. Predictor Set Expansion

## 4.1 Add Supplementary Features

```{r}
train_full <- train |> 
  select(-SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

Now, the model will be trained on the full feature set, including both the original application variables and the newly added aggregated features, to leverage all available information for optimal predictive performance.

## 4.2 3-Fold CV With Supplementary Features

```{r}
set.seed(42)
fold_ids_full <- sample(rep(1:n_folds, length.out = nrow(train_full)))

preds_lasso_full <- numeric(nrow(train_full))

for (k in 1:n_folds) {
  train_fold <- train_full[fold_ids_full != k, ]
  test_fold  <- train_full[fold_ids_full == k, ]
  
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test  <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  preds_lasso_full[fold_ids_full == k] <- 
    predict(lasso, x_test, s = "lambda.min", type = "response")
}

auc_lasso_full <- auc(roc(train_full$TARGET, preds_lasso_full, levels = c(0,1)))
auc_lasso_full
```

## 4.3 Feature Set Comparison

```{r}
feature_comparison <- tibble(
  Feature_Set = c("Application Only", "Application + Supplementary"),
  CV_AUC = c(auc_lasso_app, auc_lasso_full)
)

gt(feature_comparison)
```

The final LASSO model is trained on the full dataset, then compared against the previous LASSO model trained only on application features to evaluate the impact of adding supplementary features.

The model trained with both the application and supplementary data achieves a higher cross-validated AUC, and will be used for further tuning and final model development.

# 5. Model Optimization Using Full Feature Set

```{r}
train_model <- train_full
train_model$TARGET <- factor(train_model$TARGET,
                             levels = c(0,1),
                             labels = c("No","Yes"))
```

## 5.1 Subsample + Randomized Tuning
```{r}
set.seed(42)
tune_subset <- train_model |> 
  sample_n(10000)

ctrl_tune <- trainControl(
  method = "cv",
  number = 3,
  sampling = "smote",             # adjust for class imbalance during tuning
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(42)
lasso_tune <- train(
  TARGET ~ .,
  data = tune_subset,
  method = "glmnet",
  family = "binomial",
  metric = "ROC",
  trControl = ctrl_tune,
  tuneLength = 20
)
```

## 5.2 Final Model (5-Fold CV)

```{r}
ctrl_final <- trainControl(
  method = "cv",
  number = 5,
  sampling = "smote",            # adjust for class imbalance
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(42)
lasso_final <- train(
  TARGET ~ .,
  data = train_model,
  method = "glmnet",
  family = "binomial",
  metric = "ROC",
  trControl = ctrl_final,
  tuneGrid = lasso_tune$bestTune
)
```

```{r}
cv_auc_final <- max(lasso_final$results$ROC)
cv_auc_final
```

The predicted cross-validated AUC of the final LASSO model is 0.7551, indicating its expected discrimination performance on new, unseen data.

# 6. Predict on Test Set

```{r}
test_model <- test |> select(-SK_ID_CURR)

# generate predictions
pred_test_probs <- predict(
  lasso_final,
  newdata = test_model,
  type = "prob")[, "Yes"]
```

# 7. Create Submission File
```{r}
submission_full <- tibble(
  SK_ID_CURR = test$SK_ID_CURR,
  TARGET = pred_test_probs
)

write_csv(submission_full, "submission_lasso_full.csv")
```

The resulting submission file received a Kaggle area under the ROC curve of 0.73926 private and 0.74959 public. The private leaderboard is calculated with approximately 80% of the test data.