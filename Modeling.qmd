---
title: "Data Modeling"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

# 1. Setup

```{r}
# Load libraries
library(tidyverse)
library(pROC)
library(glmnet)
library(randomForest)
library(gt)
library(doParallel)
library(caret)
library(tibble)

options(na.action = na.pass)

data_dir <- "data-files"

# Read datasets
train <- read_csv(file.path(data_dir, "train_final.csv")) |>
  mutate(across(where(is.character), as.factor))

test <- read_csv(file.path(data_dir, "test_final.csv")) |>
  mutate(across(where(is.character), as.factor))
```

# 2. Majority Class Model

## 2.1 Overview

```{r}
TARGET_dist <- train |> 
  count(TARGET) |>
  mutate(proportion = n / sum(n))

gt(TARGET_dist)
```

The target variable is highly imbalanced, with the majority class (TARGET = 0) representing approximately 91.9% of the training data. This imbalance will be an important consideration when evaluating model performance.

## 2.2 Baseline Model Performance

```{r}
# Baseline predictions (majority class)
baseline_predictions <- rep.int(0, nrow(train))
baseline_accuracy <- mean(baseline_predictions == train$TARGET)

# Baseline probabilities (probability of TARGET = 1)
positive_class_prob <- TARGET_dist$proportion[TARGET_dist$TARGET == 1]
baseline_probs <- rep(positive_class_prob, nrow(train))

baseline_auc <- roc(train$TARGET, baseline_probs, levels = c(0,1))$auc
baseline_results <- tibble(
  Metric = c("Baseline Accuracy", "Baseline AUC"),
  Value = c(baseline_accuracy, as.numeric(baseline_auc))
)

gt(baseline_results)
```

A baseline model that always predicts the majority class achieves an accuracy of approximately 91.9% but an AUC of 0.5, indicating it cannot distinguish between the classes.

This baseline provides a reference point for evaluating predictive models, which should outperform the simple majority-class classifier to demonstrate meaningful predictive power.

# 3. Compare Candidate Models

## 3.1 Data Preparation
```{r}
raw_app_train <- read_csv(file.path(data_dir, "application_train.csv")) |>
  mutate(across(where(is.character), as.factor))

# Keep only application-based predictors
application_cols <- colnames(raw_app_train)
application_cols_in_train <- intersect(application_cols, colnames(train))

train_app <- train |>
  select(all_of(application_cols_in_train), TARGET, -SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

The initial model comparison will focus on the original application data. Supplementary data sources (bureau, previous applications, etc.) will be explored in future iterations.

## 3.2 Cross-Validation Control

```{r}
set.seed(42)
n_folds <- 3
fold_ids <- sample(rep(1:n_folds, length.out = nrow(train_app)))

# Storage for fold predictions
preds_lr <- numeric(nrow(train_app))
preds_lasso <- numeric(nrow(train_app))
preds_rf <- numeric(nrow(train_app))

# Storage for number of predictors
n_pred_lr <- n_pred_lasso <- n_pred_rf <- NA
```

Three fold cross validation is used to estimate out of sample model performance. For each fold, models are trained on two thirds of the training data and evaluated on the remaining one third. This approach provides a reliable estimate of how the models are likely to perform on unseen data while balancing computation time.

## 3.3 Parallel Processing

```{r}
n_cores <- 2
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl)
```

We set up parallel processing to speed up model fitting across folds.

## 3.4 Modeling Folds

```{r}
for (k in 1:n_folds) {
  cat("Processing fold", k, "\n")
  
  train_fold <- train_app[fold_ids != k, ]
  test_fold  <- train_app[fold_ids == k, ]
  
  # Logistic Regression
  fit_lr <- glm(TARGET ~ ., data = train_fold, family = binomial)
  preds_lr[fold_ids == k] <- predict(fit_lr, test_fold, type = "response")
  if (is.na(n_pred_lr)) n_pred_lr <- length(coef(fit_lr)) - 1
  
  # LASSO Logistic Regression
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, parallel = TRUE)
  pred_lasso_fold <- predict(lasso, x_test, s = "lambda.min", type = "response")
  preds_lasso[fold_ids == k] <- pred_lasso_fold
  if (is.na(n_pred_lasso)) {
    coefs <- coef(lasso, s = "lambda.min")
    n_pred_lasso <- sum(coefs[-1, ] != 0)
  }
  
  # Random Forest
  set.seed(42 + k)
  rf <- randomForest(TARGET ~ ., data = train_fold, ntree = 200)
  pred_rf_fold <- predict(rf, test_fold, type = "prob")[, "1"]
  preds_rf[fold_ids == k] <- pred_rf_fold
  if (is.na(n_pred_rf)) n_pred_rf <- ncol(train_fold) - 1
}
```

Each fold trains logistic regression, LASSO, and random forest models, storing out-of-fold predictions for unbiased AUC estimation.

## 3.5 Stop Parallel Cluster

```{r}
stopCluster(cl)
registerDoSEQ()
```

## 3.6 Model Comparison

```{r}
auc_lr <- auc(roc(train_app$TARGET, preds_lr, levels = c(0,1)))
auc_lasso_app <- auc(roc(train_app$TARGET, preds_lasso, levels = c(0,1)))
auc_rf <- auc(roc(train_app$TARGET, preds_rf, levels = c(0,1)))

comparison_table <- tibble(
  Model = c("Logistic", "LASSO", "Random Forest"),
  CV_AUC = c(auc_lr, auc_lasso_app, auc_rf),
  Num_Predictors = c(n_pred_lr, n_pred_lasso, n_pred_rf)
)

gt(comparison_table)
```

The LASSO model will be used for further development as it achieves the highest cross-validated AUC among candidate models while retaining fewer predictors than standard Logistic Regression.

# 4. Predictor Set Expansion

## 4.1 Add Supplementary Features

```{r}
train_full <- train |> 
  select(-SK_ID_CURR) |>
  mutate(TARGET = factor(TARGET, levels = c(0,1)))
```

Now, the model will be trained on the full feature set, including both the original application variables and the newly added aggregated features, to leverage all available information for optimal predictive performance.

## 4.2 3-Fold CV With Supplementary Features

```{r}
set.seed(42)
fold_ids_full <- sample(rep(1:n_folds, length.out = nrow(train_full)))

preds_lasso_full <- numeric(nrow(train_full))

for (k in 1:n_folds) {
  train_fold <- train_full[fold_ids_full != k, ]
  test_fold  <- train_full[fold_ids_full == k, ]
  
  x_train <- model.matrix(TARGET ~ . - 1, data = train_fold)
  y_train <- as.numeric(train_fold$TARGET) - 1
  x_test  <- model.matrix(TARGET ~ . - 1, data = test_fold)
  
  lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
  
  preds_lasso_full[fold_ids_full == k] <- 
    predict(lasso, x_test, s = "lambda.min", type = "response")
}

auc_lasso_full <- auc(roc(train_full$TARGET, preds_lasso_full, levels = c(0,1)))
auc_lasso_full
```

## 4.3 Feature Set Comparison

```{r}
feature_comparison <- tibble(
  Feature_Set = c("Application Only", "Application + Supplementary"),
  CV_AUC = c(auc_lasso_app, auc_lasso_full)
)

gt(feature_comparison)
```

The final LASSO model is trained on the full dataset, then compared against the previous LASSO model trained only on application features to evaluate the impact of adding supplementary features.

The model trained with both the application and supplementary data achieves a higher cross-validated AUC, and will be used for further tuning and final model development.

# 5. Model Optimization Using Full Feature Set

During initial model comparison, LASSO was fit using `cv.glmnet()` to evaluate cross-validated performance on different feature sets. For hyperparameter tuning and final model training, `caret::train()` with the glmnet method is used. This approach allows integration of resampling strategies (such as SMOTE), automated hyperparameter search, and structured evaluation of cross-validated AUC, making the workflow more flexible and reproducible.

```{r}
train_model <- train_full
train_model$TARGET <- factor(train_model$TARGET,
                             levels = c(0,1),
                             labels = c("No","Yes"))
```

## 5.1 Control Objects for Hyperparameter Tuning
```{r}
set.seed(42)
tune_subset <- train_model |> sample_n(5000)

ctrl_tune_no   <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

ctrl_tune_smote <- trainControl(
  method = "cv",
  number = 3,
  sampling = "smote",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)
```

### 5.2 Hyperparameter Tuning
```{r}
# Tune No Sampling
set.seed(42)
lasso_tune_no <- train(
  TARGET ~ ., data = tune_subset,
  method = "glmnet", family = "binomial",
  metric = "ROC", trControl = ctrl_tune_no,
  tuneLength = 20
)
best_no <- lasso_tune_no$bestTune

# Tune SMOTE
set.seed(42)
lasso_tune_smote <- train(
  TARGET ~ ., data = tune_subset,
  method = "glmnet", family = "binomial",
  metric = "ROC", trControl = ctrl_tune_smote,
  tuneLength = 20
)
best_smote <- lasso_tune_smote$bestTune
```

### 5.3 Sampling Comparison
```{r}
# Compare CV AUCs from tuning
cv_auc_no   <- max(lasso_tune_no$results$ROC)
cv_auc_smote <- max(lasso_tune_smote$results$ROC)

tune_comparison <- tibble(
  Model = c("LASSO (No Sampling)", "LASSO (SMOTE)"),
  `CV AUC (3-fold)` = c(cv_auc_no, cv_auc_smote)
)

# Display comparison as gt table
tune_comparison |>
  gt() |>
  fmt_number(
    columns = `CV AUC (3-fold)`,
    decimals = 4
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = `CV AUC (3-fold)` == max(`CV AUC (3-fold)`)
    )
  ) |>
  tab_header(
    title = "CV AUC Comparison: No Sampling vs SMOTE",
    subtitle = "3-Fold CV on Subsampled Data"
  )
```

# 6. Train Final Model

The LASSO model trained without sampling achieves a slighly higher cross-validated AUC compared to the model trained with SMOTE; however the difference is minimal. Given the simplicity of the no sampling approach and its slightly better performance, the final model will be trained without applying SMOTE.

## 6.1 Fit Final LASSO Model With Best Hyperparameters

```{r}
ctrl_final <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

set.seed(42)
lasso_final <- train(
  TARGET ~ .,
  data = train_model,
  method = "glmnet",
  family = "binomial",
  metric = "ROC",
  trControl = ctrl_final,
  tuneGrid = best_no
)
```

## 6.2 Final Model Performance

```{r}
# Extract CV AUC for the final model
cv_auc_final <- max(lasso_final$results$ROC)
cv_auc_final
```

The final LASSO model, trained on the full feature set without sampling, achieved a CV AUC of 0.7562. Although the fold-specific `cv.glmnet()` AUC is slightly higher (0.7693), using a single lambda across folds via `caret::train()` ensures consistent, reproducible performance and simplifies deployment, making this small reduction in AUC an acceptable trade-off.

# 7. Predict on Test Set

```{r}
test_model <- test |> select(-SK_ID_CURR)

# generate predictions
pred_test_probs <- predict(
  lasso_final,
  newdata = test_model,
  type = "prob")[, "Yes"]
```

The submission uses `SK_ID_CURR` from the test set, and the `TARGET` probabilities correspond to the positive class labeled "Yes".

# 8. Create Submission File
```{r}
submission_full <- tibble(
  SK_ID_CURR = test$SK_ID_CURR,
  TARGET = pred_test_probs
)

write_csv(submission_full, "submission_lasso_final.csv")
```

The resulting submission file received a Kaggle area under the ROC curve of 0.74213 private and 0.75230 public. The private leaderboard is calculated with approximately 80% of the test data.