---
title: "Exploratory Data Analysis"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

# Introduction

## Business Problem

Many potential borrowers lack sufficient credit histories, making it difficult for Home Credit to accurately assess their creditworthiness. This creates a challenge in extending loans to unbanked individuals while managing default risk. To continue expanding financial inclusion responsibly, Home Credit must rely on alternative data to assess repayment risk for these applicants.

## EDA Objective

The goal of this exploratory data analysis (EDA) is to develop an initial understanding of the applicant data used by Home Credit, with a focus on identifying patterns, relationships, and data quality issues that may influence loan default risk. Specifically, this analysis examines the distribution of key applicant characteristics, explores relationships between predictor variables and the `TARGET` outcome (yes/no loan default), and highlight potential challenges such as missing values, class imbalance, and outliers. Insights from this EDA will inform feature selection, data preprocessing, and modeling decisions aimed at improving credit risk assessment for applicants with limited or no traditional credit history.

# Data Exploration

## Setup

```{r}
# Load libraries
library(tidyverse)
library(tidyr)
library(dplyr)
library(gt)

data_dir <- "data-files"

# Read primary datasets
train <- read_csv(file.path(data_dir, "application_train.csv"))
test  <- read_csv(file.path(data_dir, "application_test.csv"))
```

## 1 - Majority Class Analysis

```{r}
target_proportion <- train |>
  count(TARGET) |>
  mutate(Proportion = round(n / sum(n), 4)) |>
  rename(Default = TARGET,
         Count = n)

gt(target_proportion)
```

A majority-class model that always predicts non-default would achieve an accuracy of approximately 92%.

## 2 - Correlation Analysis

```{r}
# Count types of all variables except TARGET and SK_ID_CURR
var_types <- train |>
  select(-TARGET, -SK_ID_CURR) |>
  summarise(across(everything(), ~ class(.))) |>
  pivot_longer(everything(), names_to = "Variable", values_to = "Data_Type") |>
  count(Data_Type, name = "Count")

gt(var_types)
```

Of the 120 variables in the training dataset (excluding `TARGET` and `SK_ID_CURR`), 105 are numeric and 16 are categorical.

### 2.1 - Numeric Variables

```{r}
# Select Numeric Variables
numeric_vars <- train |> select(where(is.numeric)) |>
	select(-TARGET, -SK_ID_CURR) |> 
	names()

# Calculate Correlation with Target
numeric_summary <- lapply(numeric_vars, function(x) {
  data.frame(
    Predictor = x,
    Correlation_with_Target = round(cor(train[[x]], train$TARGET, use = "complete.obs"), 4))
    }) |> 
  bind_rows()

# Rank by absolute correlation
numeric_summary <- numeric_summary |> 
	arrange(desc(abs(Correlation_with_Target)))

gt(numeric_summary)
```

The numeric variables with the highest correlation to the target variable are `EXT_SOURCE_3`, `EXT_SOURCE_2`, and `EXT_SOURCE_1`, which are all normalized scores from external data sources.

```{r}
# Prepare data for plotting
plot_data <- train |>
  select(TARGET, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) |>
  pivot_longer(
    cols = starts_with("EXT_SOURCE"),
    names_to = "Source",
    values_to = "Value"
  )

# Boxplot with fixed y-axis
ggplot(plot_data, aes(x = factor(TARGET), y = Value)) +
  geom_boxplot() +
  facet_wrap(~ Source, scales = "fixed") +  # fixed y-axis across facets
  labs(
    x = "Default (TARGET)",
    y = "EXT_SOURCE Value",
    title = "EXT_SOURCE Variables by Loan Default Status"
  ) +
  theme_minimal()
```

Across all three `EXT_SOURCE` variables, non-defaulted applicants (`TARGET` = 0) exhibit higher median values and overall distributions compared to defaulted applicants (`TARGET` = 1). This consistent separation suggests a negative association between EXT_SOURCE scores and loan default risk, indicating that higher external risk scores are associated with lower likelihood of default.

### 2.2 - Categorical Variables

```{r}
# Select categorical variables
categorical_vars <- train |>
  select(-TARGET, -SK_ID_CURR) |>
  select(where(is.character)) |>
  names()

# Summarize categorical variables
cat_summary <- lapply(categorical_vars, function(x) {
  temp <- train |>
    group_by(.data[[x]]) |>
    summarise(
      Default_Rate = mean(TARGET, na.rm = TRUE),
      .groups = "drop"
    )
  
  data.frame(
    Predictor = x,
    Max_Proportion_Diff = max(temp$Default_Rate) - min(temp$Default_Rate)
  )
}) |>
  bind_rows() |>
  arrange(desc(Max_Proportion_Diff))

gt(cat_summary)
```

`NAME_INCOME_TYPE` shows the strongest separation in default rates across its categories, suggesting it may be the most informative categorical predictor.

```{r}
plot_data <- train |>
  group_by(NAME_INCOME_TYPE) |>
  summarise(
    Default_Rate = mean(TARGET, na.rm = TRUE),
    Count = n(),
    .groups = "drop"
  )

# Bar chart of default rate
ggplot(plot_data, aes(x = reorder(NAME_INCOME_TYPE, Default_Rate), y = Default_Rate)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = scales::percent(Default_Rate, accuracy = 1)), 
            vjust = -0.38) +
  labs(title = "Default Rate by Income Type",
       x = "Income Type",
       y = "Default Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Loan default rate is highest amongst "Unemployed" and "Maternity Leave" income types at 40% and 36% respectively, while "Student" and "Businessman" have default rates of 0%.

## 3 - Missing Data

```{r}
# Add a placeholder SalePrice to test
test <- test |> 
  mutate(TARGET = NA)

# Combine train and test
combined <- bind_rows(
  train |>mutate(dataset = "train"),
  test  |> mutate(dataset = "test")
)
```

```{r}
#count NAs
count_missings <- function(x) sum(is.na(x))

missing_summary <- combined |>
  select(-TARGET, -SK_ID_CURR) |>  # exclude TARGET and ID
  summarize_all(~ sum(is.na(.))) |>
  pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "Count"
  ) |>
  filter(Count > 0) |>
  mutate(Proportion = round(Count / nrow(combined), 4)) |>
  arrange(desc(Count))

gt(missing_summary) |>
  tab_header(
    title = "Missing Data Summary",
  )
```

47 variables included in the missing data summary share a common description: "Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor." For these building-level characteristics, NULL values can reasonably be interpreted as the feature being absent or not applicable. For example, a null value for `BASEMENTAREA_AVG` likely indicates that the residence does not have a basement.

```{r}
building_level_variables <- c(
  # AVG
  "APARTMENTS_AVG", "BASEMENTAREA_AVG", "YEARS_BEGINEXPLUATATION_AVG",
  "YEARS_BUILD_AVG", "COMMONAREA_AVG", "ELEVATORS_AVG", "ENTRANCES_AVG",
  "FLOORSMAX_AVG", "FLOORSMIN_AVG", "LANDAREA_AVG", "LIVINGAPARTMENTS_AVG",
  "LIVINGAREA_AVG", "NONLIVINGAPARTMENTS_AVG", "NONLIVINGAREA_AVG",
  # MODE
  "APARTMENTS_MODE", "BASEMENTAREA_MODE", "YEARS_BEGINEXPLUATATION_MODE",
  "YEARS_BUILD_MODE", "COMMONAREA_MODE", "ELEVATORS_MODE", "ENTRANCES_MODE",
  "FLOORSMAX_MODE", "FLOORSMIN_MODE", "LANDAREA_MODE", "LIVINGAPARTMENTS_MODE",
  "LIVINGAREA_MODE", "NONLIVINGAPARTMENTS_MODE", "NONLIVINGAREA_MODE",
  "FONDKAPREMONT_MODE", "HOUSETYPE_MODE", "TOTALAREA_MODE",
  "WALLSMATERIAL_MODE", "EMERGENCYSTATE_MODE",
  # MEDI
  "APARTMENTS_MEDI", "BASEMENTAREA_MEDI", "YEARS_BEGINEXPLUATATION_MEDI",
  "YEARS_BUILD_MEDI", "COMMONAREA_MEDI", "ELEVATORS_MEDI", "ENTRANCES_MEDI",
  "FLOORSMAX_MEDI", "FLOORSMIN_MEDI", "LANDAREA_MEDI", "LIVINGAPARTMENTS_MEDI",
  "LIVINGAREA_MEDI", "NONLIVINGAPARTMENTS_MEDI", "NONLIVINGAREA_MEDI"
)

missing_summary_2 <- combined |>
  select(-TARGET, -SK_ID_CURR, -building_level_variables) |>  # exclude TARGET, ID and building-level variables
  summarize_all(~ sum(is.na(.))) |>
  pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "Count"
  ) |>
  filter(Count > 0) |>
  mutate(Proportion = round(Count / nrow(combined), 4)) |>
  arrange(desc(Count))

gt(missing_summary_2) |>
  tab_header(
    title = "Missing Data Summary",
  )
```

Other variables in the summary, such as `OWN_CAR_AGE` (which indicates the age of the client's car), may be NULL because the client does not own a car.

For variables where NULL represents genuinely missing information rather than the absence of a feature, missing values should be imputedâ€”using the median for numeric variables and the most frequent value (mode) for categorical variables.

In general, variables with substantial missingness might be considered for removal. However, `EXT_SOURCE_1` is highly predictive of the target variable, so rather than removing it, missing values should be imputed.
