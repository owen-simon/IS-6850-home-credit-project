---
title: "Exploratory Data Analysis"
author: "Owen Simon"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false
---

# Introduction

## Business Problem

Many potential borrowers lack sufficient credit histories, making it difficult for Home Credit to accurately assess their creditworthiness. This creates a challenge in extending loans to unbanked individuals while managing default risk. To continue expanding financial inclusion responsibly, Home Credit must rely on alternative data to assess repayment risk for these applicants.

## EDA Objective

The goal of this exploratory data analysis (EDA) is to develop an initial understanding of the applicant data used by Home Credit, with a focus on identifying patterns, relationships, and data quality issues that may influence loan default risk. Specifically, this analysis examines the distribution of key applicant characteristics, explores relationships between predictor variables and the `TARGET` outcome (yes/no loan default), and highlight potential challenges such as missing values, class imbalance, and outliers. Insights from this EDA will inform feature selection, data preprocessing, and modeling decisions aimed at improving credit risk assessment for applicants with limited or no traditional credit history.

# Data Exploration

## Setup

```{r}
# Load libraries
library(tidyverse)
library(tidyr)
library(dplyr)
library(gt)

data_dir <- "data-files"

# Read primary datasets
train <- read_csv(file.path(data_dir, "application_train.csv"))
test  <- read_csv(file.path(data_dir, "application_test.csv"))
dict <- read_csv(file.path(data_dir, "HomeCredit_columns_description.csv"))
```

## 1 - Majority Class Analysis

```{r}
target_proportion <- train |>
  count(TARGET) |>
  mutate(Proportion = round(n / sum(n), 4)) |>
  rename(Default = TARGET,
         Count = n)

gt(target_proportion)
```

A majority-class model that always predicts non-default would achieve an accuracy of approximately 92%.

## 2 - Correlation Analysis

```{r}
# Count types of all variables except TARGET and SK_ID_CURR
var_types <- train |>
  select(-TARGET, -SK_ID_CURR) |>
  summarise(across(everything(), ~ class(.))) |>
  pivot_longer(everything(), names_to = "Variable", values_to = "Data_Type") |>
  count(Data_Type, name = "Count")

gt(var_types)
```

Of the 120 variables in the training dataset (excluding `TARGET` and `SK_ID_CURR`), 105 are numeric and 16 are categorical.

### 2.1 - Numeric Variables

```{r}
# Select Numeric Variables
numeric_vars <- train |> select(where(is.numeric)) |>
	select(-TARGET, -SK_ID_CURR) |> 
	names()

# Calculate Correlation with Target
numeric_summary <- lapply(numeric_vars, function(x) {
  data.frame(
    Predictor = x,
    Correlation_with_Target = round(cor(train[[x]], train$TARGET, use = "complete.obs"), 4))
    }) |> 
  bind_rows()

# Rank by absolute correlation
numeric_summary <- numeric_summary |> 
	arrange(desc(abs(Correlation_with_Target)))

gt(numeric_summary)
```

The numeric variables with the highest correlation to the target variable are `EXT_SOURCE_3`, `EXT_SOURCE_2`, and `EXT_SOURCE_1`, which are all normalized scores from external data sources.

```{r}
# Prepare data for plotting
plot_data <- train |>
  select(TARGET, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) |>
  pivot_longer(
    cols = starts_with("EXT_SOURCE"),
    names_to = "Source",
    values_to = "Value"
  )

# Boxplot with fixed y-axis
ggplot(plot_data, aes(x = factor(TARGET), y = Value)) +
  geom_boxplot() +
  facet_wrap(~ Source, scales = "fixed") +  # fixed y-axis across facets
  labs(
    x = "Default (TARGET)",
    y = "EXT_SOURCE Value",
    title = "EXT_SOURCE Variables by Loan Default Status"
  ) +
  theme_minimal()
```

Across all three `EXT_SOURCE` variables, non-defaulted applicants (`TARGET` = 0) exhibit higher median values and overall distributions compared to defaulted applicants (`TARGET` = 1). This consistent separation suggests a negative association between EXT_SOURCE scores and loan default risk, indicating that higher external risk scores are associated with lower likelihood of default.

### 2.2 - Categorical Variables

```{r}
# Select categorical variables
categorical_vars <- train |>
  select(-TARGET, -SK_ID_CURR) |>
  select(where(is.character)) |>
  names()

# Summarize categorical variables
cat_summary <- lapply(categorical_vars, function(x) {
  temp <- train |>
    group_by(.data[[x]]) |>
    summarise(
      Default_Rate = mean(TARGET, na.rm = TRUE),
      .groups = "drop"
    )
  
  data.frame(
    Predictor = x,
    Max_Proportion_Diff = max(temp$Default_Rate) - min(temp$Default_Rate)
  )
}) |>
  bind_rows() |>
  arrange(desc(Max_Proportion_Diff))

gt(cat_summary)
```

`NAME_INCOME_TYPE` shows the strongest separation in default rates across its categories, suggesting it may be the most informative categorical predictor.

```{r}
plot_data <- train |>
  group_by(NAME_INCOME_TYPE) |>
  summarise(
    Default_Rate = mean(TARGET, na.rm = TRUE),
    Count = n(),
    .groups = "drop"
  )

# Bar chart of default rate
ggplot(plot_data, aes(x = reorder(NAME_INCOME_TYPE, Default_Rate), y = Default_Rate)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = scales::percent(Default_Rate, accuracy = 1)), 
            vjust = -0.38) +
  labs(title = "Default Rate by Income Type",
       x = "Income Type",
       y = "Default Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Loan default rate is highest amongst "Unemployed" and "Maternity Leave" income types at 40% and 36% respectively, while "Student" and "Businessman" have default rates of 0%.

## 3 - Missing Data

```{r}
# Add a placeholder SalePrice to test
test <- test |> 
  mutate(TARGET = NA)

# Combine train and test
combined <- bind_rows(
  train |>mutate(dataset = "train"),
  test  |> mutate(dataset = "test")
)
```

Before examining missing values, the training and test datasets were combined into a single dataset. A placeholder TARGET column was added to the test set, and a dataset indicator was created to track the origin of each row. This setup ensures that missing value analysis can be performed consistently across both datasets.

```{r}
#count NAs
count_missings <- function(x) sum(is.na(x))

missing_summary <- combined |>
  select(-TARGET, -SK_ID_CURR) |>  # exclude TARGET and ID
  summarize_all(~ sum(is.na(.))) |>
  pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "Count"
  ) |>
  filter(Count > 0) |>
  mutate(Proportion = round(Count / nrow(combined), 4)) |>
  arrange(desc(Count))

gt(missing_summary) |>
  tab_header(
    title = "Missing Data Summary",
  )
```

47 variables included in the missing data summary share a common description: "Normalized information about building where the client lives, What is average (\_AVG suffix), modus (\_MODE suffix), median (\_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor." For these characteristics, NULL values can reasonably be interpreted as the feature being absent or not applicable. For example, a NULL in BASEMENTAREA_AVG likely indicates that the residence does not have a basement. Because the variables are normalized on a 0–1 scale, replacing NULLs with 0 is both intuitive and semantically meaningful: 0 effectively encodes “feature not present,” preserving the interpretation of the normalized score without introducing artificial averages.

```{r}
building_level_variables <- dict |>
  filter(Table == "application_{train|test}.csv",
         Description == "Normalized information about building where the client lives, What is average (_AVG suffix), modus (_MODE suffix), median (_MEDI suffix) apartment size, common area, living area, age of building, number of elevators, number of entrances, state of the building, number of floor") |>
  pull(Row)

missing_summary_2 <- combined |>
  select(-TARGET, -SK_ID_CURR, -building_level_variables) |>  # exclude TARGET, ID and building-level variables
  summarize_all(~ sum(is.na(.))) |>
  pivot_longer(
    cols = everything(),
    names_to = "Variable",
    values_to = "Count"
  ) |>
  filter(Count > 0) |>
  mutate(Proportion = round(Count / nrow(combined), 4)) |>
  arrange(desc(Count))

gt(missing_summary_2) |>
  tab_header(
    title = "Missing Data Summary",
  )
```

Other variables in the summary, such as `OWN_CAR_AGE` (which indicates the age of the client's car), may be NULL because the client does not own a car. In such cases, NULL values should be replaced with 0.

For variables where NULL represents genuinely missing information rather than the absence of a feature, missing values should be imputed to avoid losing information. Numeric variables should be imputed using the median, while categorical variables should use the most frequent value (mode).

Variables with substantial missingness may be candidates for removal; however, some variables, such as `EXT_SOURCE_1`, are highly predictive of the target. In these cases, imputing missing values is preferable to dropping the variable, preserving its predictive value in modeling.

## - 4 Data Problems

### Normalized Variables

```{r}
# Select variables marked as normalized in the data dictionary
normalized_tag <- dict |>
  filter(Table == "application_{train|test}.csv",
         Special == "normalized") |>
  pull(Row)

# Subset train
normalized <- train |> select(all_of(normalized_tag))

# Check data types
var_types <- sapply(normalized, class)
var_types
```

Most variables labeled as “normalized” in the data dictionary are numeric and can be interpreted on a 0–1 scale. However, four of the 47 building-level variables—FONDKAPREMONT_MODE, HOUSETYPE_MODE, WALLSMATERIAL_MODE, and EMERGENCYSTATE_MODE—are actually categorical (character) variables, not numeric. These variables are therefore incorrectly labeled as “normalized.”

Previously, it was suggested that all building-level variables have their NULLs imputed with 0. For these four categorical variables, however, imputing NULLs with 0 is inappropriate and would misrepresent the data. Missing values should instead be handled using mode imputation or other categorical encoding strategies to preserve their meaning.

```{r}
# Update Normalized Variables
normalized <- normalized |>
  select(-FONDKAPREMONT_MODE, 
         -HOUSETYPE_MODE, 
         -WALLSMATERIAL_MODE, 
         -EMERGENCYSTATE_MODE)

summary(normalized)
```

After excluding the four incorrectly labeled categorical variables, all remaining variables tagged as “normalized” are numeric and have observed values bounded between 0 and 1. This confirms that these variables are consistently scaled and that their normalization aligns with the data dictionary’s description.

### Numeric Variables

```{r}
# check all numeric variables (exclude normalized since they have been checked)
numeric_error_check <- train |>
  select(
    where(is.numeric),
    -TARGET,
    -SK_ID_CURR,
    -all_of(names(normalized))
  )

summary(numeric_error_check)
```

Outliers and extreme values:

-   CNT_CHILDREN has a maximum of 19, which is unusually high.

-   AMT_INCOME_TOTAL reaches 117,000,000, far above the 3rd quartile of 202,500.

-   AMT_ANNUITY and AMT_CREDIT show similar extreme high values compared to their medians.

-   DAYS_EMPLOYED contains a positive maximum (365,243), likely a placeholder for missing values.

Unusual distributions:

-   CNT_FAM_MEMBERS has a maximum of 20, which may indicate data errors or extended household counts.

-   Social circle observation variables (OBS_30_CNT_SOCIAL_CIRCLE, OBS_60_CNT_SOCIAL_CIRCLE) have extreme max values (e.g., 348), suggesting outliers.

-   Most FLAG_DOCUMENT\_\* variables are heavily skewed toward 0, indicating rare document submissions.

Expected conventions:

DAYS_BIRTH and other day-based variables are negative, representing days before the loan application.

### Categorical

```{r}
# Unique levels per categorical variable
cat_levels <- lapply(categorical_vars, function(x){
  train |>
    select(.data[[x]]) %>% distinct()})

cat_levels[["CODE_GENDER"]]
```